Index: recrawl_failed.py
===================================================================
--- recrawl_failed.py	(revision 0)
+++ recrawl_failed.py	(revision 0)
@@ -0,0 +1,81 @@
+#coding: utf-8
+
+import time
+
+try:
+  import cPickle as pickle
+except:
+  import pickle
+
+from pymongo import MongoClient, UpdateOne
+
+
+from le_crawler.common.logutil import Log
+from le_crawler.proto.crawl.ttypes import CrawlStatus
+from le_crawler.core.scheduler_client import SchedulerClient
+
+class Recrawler(object):
+  def __init__(self):
+    self.logger_ = Log('recrawl_failed', 'log/recrawl_failed.log').log
+    self.client_ = SchedulerClient('65.255.32.210', 8088)
+    self.client_.open(self.logger_)
+    self.exit_ = False
+    self._init_client()
+
+  def _init_client(self):
+    try:
+      client = MongoClient('10.120.1.61:9220,10.120.1.62:9220,10.120.1.63:9220')
+      self._db = client.admin
+      self._db.authenticate('admin', 'NzU3ZmU4YmFhZDg')
+      self._collection = self._db.recrawl_info
+    except Exception, e:
+      self._collection = None
+      self._logger.exception('failed to connect to mongodb...')
+
+  def run(self):
+    while not self.exit_:
+      now = int(time.time())
+      update_items = []
+      docs = []
+      self.logger_.info('begin to scan.')
+      #print 'begin to scan.....'
+      for item in self._collection.find({'next_schedule_time': {'$lt': now}}):
+      #for item in self._collection.find({}):
+        time_now = int(time.time())
+        retry_times = item.get('retry_times', 0) + 1
+        if retry_times > 3:
+          self.logger_.info('retry max_times: %s, remove url: %s' % (retry_times, item['url']))
+          try:
+            self._collection.remove({'url': item['url']})
+          except:
+            self.logger_.info('failed remove url: %s' % item['url'])
+          continue
+        doc_slim = item.get('crawl_doc_slim', None)
+        if not doc_slim:
+          continue
+        docs.append(pickle.loads(doc_slim.encode('utf-8')))
+        schedule_delta_time = 3600 * (retry_times + 3)
+        next_schedule_time = time_now + schedule_delta_time
+        update_item = UpdateOne({'url': item['url']},{'$set': {'next_schedule_time': next_schedule_time,'status': CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.SCHEDULING), 'retry_times': retry_times}}, upsert=True)
+        update_items.append(update_item)
+        self.logger_.info('recrawl url: %s, retry_times: [%s], next_schedule_time: [%s]' % (item['url'], retry_times, next_schedule_time))
+        if len(docs) >= 50:
+          self.logger_.info('recawl docs, [%s]', len(docs))
+          self.client_.set_crawldocs_local(docs)
+          self._collection.bulk_write(update_items, ordered=False)
+          update_items = []
+          docs = []
+      if docs:
+        self.logger_.info('recrawl docs, [%s]', len(docs))
+        self.client_.set_crawldocs_local(docs)
+      if update_items:
+        self._collection.bulk_write(update_items, ordered=False)
+      self.logger_.info('finish recrawl.....')
+      #print 'finish recrawl.....'
+      time.sleep(5 * 60)
+    self._db.logout()
+
+
+if __name__ == '__main__':
+  recrawler = Recrawler()
+  recrawler.run()
Index: le_crawler/core/scheduler.py
===================================================================
--- le_crawler/core/scheduler.py	(revision 114546)
+++ le_crawler/core/scheduler.py	(working copy)
@@ -11,6 +11,7 @@
 except:
   import pickle
 from threading import Thread
+from random import choice
 
 # from scrapy.utils.misc import load_object
 from scrapy.utils.reqser import request_to_dict, request_from_dict
@@ -48,7 +49,7 @@
   def open(self, spider):
     self.spider_ = spider
     self.logger_ = spider.logger_
-    self.len_start_urls_ = len(self.spider_.start_urls)
+    self.len_start_urls_ = self.spider_.start_size
     self.logger_.info('amount of start urls: %s', self.len_start_urls_)
     self.scheduler_client_.open(self.logger_)
     if self.idle_before_close < 0:
@@ -145,6 +146,10 @@
         flush_docs.append(CrawlDocSlim(url=doc.url,
                                        crawl_doc=thrift_util.thrift_to_str(doc),
                                        priority=self._cal_priority(doc)))
+        self.spider_.update_recrawl_info(url=doc.url,
+                                         data={'next_schedule_time': int(time.time()) + 7200,
+                                               'retry_times': 0,
+                                               'crawl_doc_slim': pickle.dumps(crawl_doc_slim)})
       except:
         self.logger_.exception('failed to get flush doc from cache.')
     if flush_docs:
@@ -229,3 +234,36 @@
   def has_pending_requests(self):
     return len(self) > 0
 
+
+class SchedulerYoutube(CrawlDocScheduler):
+  def __init__(self, thrift_client, idle_before_close, crawler):
+    CrawlDocScheduler.__init__(self, thrift_client, idle_before_close, crawler)
+    self._keys = ['AIzaSyADAw1LV8-DmiqJNvYD7qxTRn7VclazxAE',
+                  'AIzaSyDR82r3LDFYgXAnio126YwtkWWcOfwrcDM',
+                  'AIzaSyAAvUAixwoB2XtPsuX-i6aq64QStKczcag',
+                  'AIzaSyDNW5VmzjLzzsxOWcLhse8zXZWAyHcbggM',
+                  'AIzaSyDklVqYGpjE3nOUDIOuc5fNRrdFr-t7T9g',
+                  'AIzaSyCLiSdR3CBH2AcgnwPEovag88BrPCfyhPA',
+                  'AIzaSyCObEV-VM_xecAQGROfi8RA9qB5eLqxFWc']
+    self.cache_input_max_ = 50
+    self.cache_upload_max_ = 50
+
+
+  def _add_key(self, url):
+    if not url:
+      return None
+    if 'key=' in url:
+      self._logger.error('url already has key')
+      return url
+    key = choice(self._keys)
+    return '%s&key=%s' % (url, key)
+
+
+  def next_request(self):
+    request = super(SchedulerYoutube, self).next_request()
+    if request:
+      replace_url = self._add_key(request.url)
+      #print 'replace_url:', replace_url
+      request = request.replace(url=replace_url)
+    return request
+
Index: le_crawler/spiders/lejian_crawler.py
===================================================================
--- le_crawler/spiders/lejian_crawler.py	(revision 114546)
+++ le_crawler/spiders/lejian_crawler.py	(working copy)
@@ -57,6 +57,7 @@
       self.extend_map_h_ = ExtendMapHandler.get_instance(LejianCrawler.start_url_loader,
                                                          module_path='le_crawler.common.lejian_video_settings', loger=self.logger_)
       self.finished_count = 0
+      self.start_size = len(LejianCrawler.start_urls)
       self.callback_map_ = {PageType.HOME:        self.parse_home,
                             PageType.CHANNEL:     self.parse_channel,
                             PageType.HUB:         self.parse_list,
@@ -80,12 +81,19 @@
         self._request_seen = self._request_seen_debug
         self._insert_md5 = self._insert_md5_debug
         self.update_status = self._update_status_debug
+        self._update_recrawl_info = self.update_recrawl_info_debug
+        self.remove_recrawl_info = self.remove_recrawl_info_debug
       else:
         self.mongo_client_ = MongoClient('10.180.91.41:9224,10.180.91.115:9224,10.180.91.125:9224')
         self.mongo_client_.admin.authenticate('admin', 'NjlmNTdkNGQ4OWY')
+        self._recrawl_collection = self.mongo_client_.crawl.recrawl_failed_info
+        self._ensure_indexs()
         self.inserter_ = UrlMd5Inserter(Log('urlmd5', log_path='../log/urlmd5.log', log_level=logging.DEBUG).log)
 
+    def _ensure_indexs(self):
+      self._recrawl_collection.create_index('url', unique=True)
 
+
     def _update_status_debug(self, url, data):
       pass
 
@@ -97,7 +105,13 @@
     def _insert_md5_debug(self, crawl_doc, crawl_item):
       pass
 
+    def update_recrawl_info_debug(self, url, data):
+      pass
 
+    def remove_recrawl_info_debug(self, url):
+      pass
+
+
     def _gen_videos_from_list(self, base_url, data, crawl_doc):
       user = None
       if 'user' in data:
@@ -153,7 +167,26 @@
           setattr(video, k, v)
       return video
 
+  def update_recrawl_info(self, url, data):
+    if not url:
+      return
+    data.update({'update_time': int(time.time())})
+    #print 'update recrawl info, url: %s' % url
+    try:
+      self._recrawl_collection.update({'url': url}, {'$set': data}, upsert=True)
+    except Exception, e:
+      self.logger_.exception('Failed update recrawl info:[%s]' % url)
 
+  def remove_recrawl_info(self, url):
+    if not url:
+      return
+    #print 'remove recrawl info, url: %s' % url
+    try:
+      self._recrawl_collection.remove({'url': url})
+    except Exception, e:
+      self.logger_.exception('Failed remove recrawl_info, url:[%s]' % url)
+
+
     def update_status(self, doc, status):
       data = {'url': doc.url,
               'status': status,
@@ -247,6 +280,7 @@
       try:
         self.logger_.info('parse home url: %s', doc.url)
         self.update_status(doc, CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.DOWNLOADED))
+        self.remove_recrawl_info(doc.url)
         page = response.body.decode(response.encoding, 'ignore')
         doc_type = response.meta.get('doc_type', CrawlDocType.HUB_HOME)
         prefix = doctype_prefix_map.get(doc_type, '')
@@ -274,6 +308,7 @@
       doc = response.meta.get('crawl_doc')
       self.logger_.info('parse channel url: %s', doc.url)
       self.update_status(doc, CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.DOWNLOADED))
+      self.remove_recrawl_info(doc.url)
       try:
         page = response.body.decode(response.encoding, 'ignore')
         sub_category_num = response.meta.get('sub_category_num', 0)
@@ -305,6 +340,7 @@
       doc = response.meta.get('crawl_doc')
       self.logger_.info('parse order list url: %s', doc.url)
       self.update_status(doc, CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.DOWNLOADED))
+      self.remove_recrawl_info(doc.url)
       try:
         page = response.body.decode(response.encoding, 'ignore')
         sta, order_select, url_map = self.extend_map_h_.extract_orderlist_map(body=page, pageurl=doc.url)
@@ -338,6 +374,7 @@
       doc = response.meta.get('crawl_doc')
       self.logger_.info('parse_list url: %s', doc.url)
       self.update_status(doc, CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.DOWNLOADED))
+      self.remove_recrawl_info(doc.url)
       response = self.extend_map_h_.assemble_html(doc.url, response)
       try:
         page = response.body.decode(response.encoding, 'ignore')
@@ -419,6 +456,7 @@
         return
       doc = response.meta.get('crawl_doc')
       self.update_status(doc, CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.DOWNLOADED))
+      self.remove_recrawl_info(doc.url)
       url = doc.url
       self.logger_.info('parse_page url: %s referer--> %s' % (url, doc.in_links[0].url if doc.in_links else None))
       response = self.extend_map_h_.assemble_html(url, response)
