#!/usr/bin/python
# coding=utf8
# Copyright 2014 LeTV Inc. All Rights Reserved.
# not thread safe

__author__ = 'guoxiaohe@letv.com'

# this cls is using for crawler backgroud request
# store db, store by domain

import os
import time
import random
import string
import hashlib
import threading
from threading import Thread
# import traceback
try:
  import cPickle as pickle
except:
  import pickle

from logutil import Log
from sharedb import ShareDB, DBOption
from url_domain_parser import query_domain_from_url
from le_crawler.proto.crawl.ttypes import CrawlDocType


fresh_docs = set(range(CrawlDocType.HUB_FRESH_MIN, CrawlDocType.HUB_FRESH_MAX))


class CrawlerDBManger(object):
  STOP_READ = 1
  STOP_WRITE = (1 << 1)

  def __init__(self, db_path, logger=None):
    self.logger_ = logger or Log('crawlerdb_log', 'crawler_sch_db.log').log
    self._list_logger = Log('list_logger', 'log/list_recrawler.log').log
    self.db_base_path_ = db_path
    self.db_dict_ = {}
    self._load_dbs(self.db_base_path_)
    self.exit_sig_ = False
    self.max_schedule_delay_ = 0
    self.min_schedule_delay_ = 24 * 60 * 60  # 1 day
    self._load_from_file()
    Thread(target=self._recall_hub_docs).start()
    self.db_create_lock_ = threading.Lock()


  def _load_from_file(self):
    self.logger_.info('loading data from cache file...')
    if not os.path.isfile('data/refresh_pool.data'):
      self.logger_.error('refresh pool file not found, create one instead.')
      self.docs_pool_ = {}
    else:
      with open('data/refresh_pool.data', 'r') as f:
        self.docs_pool_ = pickle.load(f)
    self.logger_.info('load data from cache file finished.')


  def _dump_to_file(self):
    self.logger_.info('dumping data...')
    if not os.path.isdir('data'):
      os.mkdir('data')
    with open('data/refresh_pool.data', 'w') as f:
      pickle.dump(self.docs_pool_, f, 2)
    self.logger_.info('dump data finished.')


  def stop(self):
    self.logger_.info('stopping crawler database manager...')
    self.exit_sig_ = True
    self._dump_to_file()
    self.logger_.info('stop crawler database manager finished.')


  def _gather_info(self):
    databases = self.db_dict_.copy()
    statistic = 'Scheduler details:'
    for domain, database in databases.iteritems():
      statistic += '\n\t%16s: %s' % (domain, database.size())
    self.logger_.info(statistic)


  def _recall_hub_docs(self):
    def put_into_db(item):
      key, doc = item
      db = self._get_db(doc.url)
      if db:
        doc.discover_time = int(time.time())
        db.put(key, doc.crawl_doc)
        self._list_logger.info('recalling list url: [%s]' % doc.url)

    while not self.exit_sig_:
      if self.docs_pool_:
        docs = self.docs_pool_.values()
        map(put_into_db, docs)
        self.logger_.info('finished recalling [%s] hub docs into scheduler.' % len(docs))
        self._gather_info()
      time.sleep(30 * 60)


  def _load_dbs(self, db_path):
    if not os.path.isdir(db_path):
      os.mkdir(db_path)
    dbids = os.listdir(db_path)
    for dbid in dbids:
      tmpdb = self._create_db(dbid)
      if tmpdb:
        self.db_dict_[dbid] = tmpdb
        self.logger_.info('Load db [%s]' % dbid)
      else:
        self.logger_.error('Load db [%s] failed' % dbid)
    self.logger_.info('db loaded total: [%d]' % len(self.db_dict_))
    return True


  def _create_db(self, dbid):
    db_path = os.path.join(self.db_base_path_, dbid)
    op = DBOption()
    domain_db = ShareDB(db_path, self.logger_, dboption=op)
    self.logger_.info('created db [%s] to [%s] ' % (dbid, db_path))
    return domain_db


  def _get_db(self, url):
    dbid = query_domain_from_url(url)
    if not dbid:
      self.logger_.error('failed parse domain from url, %s', url)
      return None
    if dbid in self.db_dict_:
      return self.db_dict_[dbid]
    else:
      self.db_create_lock_.acquire()
      if dbid in self.db_dict_:
        self.db_create_lock_.release()
        return self.db_dict_[dbid]
      self.db_dict_[dbid] = self._create_db(dbid)
      self.db_create_lock_.release()
      assert self.db_dict_[dbid], 'Create db [%s] failed' % dbid
      return self.db_dict_[dbid]


  def _get_store_key(self, url, priority=6):
    return '%s_%s' % (string.zfill(priority, 6), hashlib.md5(url).hexdigest())


  def put(self, doc):
    if not doc:
      return True
    try:
      db = self._get_db(doc.url)
      if not db:
        self.logger_.debug('failed get db id, url: %s', doc.url)
        return False
      if doc.priority in fresh_docs and doc.url not in self.docs_pool_:
        key = self._get_store_key(doc.url, 0)
        self.docs_pool_[doc.url] = (key, doc)
      else:
        key = self._get_store_key(doc.url, doc.priority)
      db.put(key, doc.crawl_doc)
      self.logger_.debug('put into db, key: [%s], %s' % (key, doc.url))
    except:
      self.logger_.exception('failed to put request')
      return False


  def mixin_list(self, data):
    # data为list，其元素也是list
    # 首先按照元素长度降序排列
    # 对于各个list，根据步长step逆序插入结果list中
    # 输出为list
    if not data:
      return data
    data.sort(cmp=lambda x, y: len(y) - len(x))
    result = data[0]
    step = 1
    for item in data[1:]:
      index = len(result)
      for i in item:
        result.insert(index, i)
        index -= step
      if item:
        step += 1
    self.logger_.debug('end mixin')
    return result


  def get_crawldocs(self, batch_num=1024):
    self.logger_.info('getting requests: [%s]', batch_num)
    data = []
    if not batch_num or not self.db_dict_:
      return data
    average = batch_num / len(self.db_dict_)
    remainder = batch_num % len(self.db_dict_) + average
    # batch_num = (batch_num + len(self.db_dict_) - 1 ) / batch_num
    # self.logger_.debug('batch num: %s' % batch_num)
    selected = random.choice(range(len(self.db_dict_)))
    spare = total = 0
    for idx, dbid in enumerate(self.db_dict_.keys()):
      actual_num = (average if idx != selected else remainder) + spare
      batch_data = self.db_dict_[dbid].batch_get(actual_num)
      self.logger_.debug('got request from [%s]: [%d]' % (dbid, len(batch_data)))
      spare = actual_num - len(batch_data)
      self.db_dict_[dbid].batch_delete([k for (k, v) in batch_data])
      total += len(batch_data)
      data.append([v for (k, v) in batch_data])
    self.logger_.info('batch num actual: %s' % total)
    data = self.mixin_list(data)
    return data


  # return all the db status
  def get_status(self):
    pass


  # write the request to db
  # the api for request_manger ,
  # input: request is [requests, request,]
  def set_crawldocs(self, crawldocs):
    try:
      self.logger_.info('writing requests: [%s]', len(crawldocs))
      valid_doc_len = 0
      for doc in crawldocs:
        self.put(doc)
        self.logger_.info('url added, %s', doc.url)
        valid_doc_len += 1
      self.logger_.info('finished writing requests: [%s]', valid_doc_len)
    except:
      self.logger_.exception('failed to write requests')

