#!/usr/bin/python
#
# Copyright 2014 LeTV Inc. All Rights Reserved.

"""
get base crawler items to fill
using unicode as middle code
decoing item with utf8
in crawler we recommend using coding unicode
when output data should using utf8
"""

__author__ = 'guoxiaohe@letv.com (Guo XiaoHe)'

import json
import traceback
import base64
from scrapy.item import Item, Field
from scrapy.loader import ItemLoader
from scrapy.loader.processors import MapCompose, TakeFirst, Join
import time

from ..base.thrift_util import thrift_to_str
from ..proto.crawl.ttypes import Request, Response, RedirectInfo
from docid_generator import gen_docid

# all item should first alig to unicode,
# output should encod utf8
class ItemType(object):
  CRAWL_DOC = 0X11
  WEB_PAGE = 0x12
  PICS_DOC = 0x13
  IMAGE = 0x14


def fill_base_item(response, item, except_key=[]):
  crawl_doc = response.meta.get('crawl_doc')
  if not crawl_doc:
    print 'Empty crawl_doc !!!!'
    return

  crawl_doc.id = gen_docid(crawl_doc.url)

  crawl_doc.request = Request()
  crawl_doc.request.raw_url = response.request.meta.get('Rawurl')

  crawl_doc.response = Response()
  crawl_doc.response.url = crawl_doc.url
  crawl_doc.response.body = response.body.decode(response.encoding, 'ignore').encode('utf-8')
  crawl_doc.response.header = response.headers.to_string()
  crawl_doc.response.return_code = response.status
  crawl_doc.response.redirect_info = RedirectInfo()
  crawl_doc.response.redirect_info.redirect_urls = response.meta.get('redirect_urls')

  item['crawl_doc'] = encode_item(crawl_doc)
  return item


def encode_item(obj):
  if isinstance(obj, unicode):
    return obj.encode('utf-8')
  if isinstance(obj, dict):
    for (k, v) in obj.items():
      obj[encode_item(k)] = encode_item(v)
  if hasattr(obj, '__iter__'):
    for idx, v in enumerate(obj):
      obj[idx] = encode_item(v)
  if hasattr(obj, "__dict__"):
    for key, value in obj.__dict__.iteritems():
      if not callable(value):
        setattr(obj, key, encode_item(value))
  return obj


# post item process
def process_item(item, encoding):
  if not item:
    return item
  try:
    for k in item.keys():
      if isinstance(item[k], str) or k == 'page':
        item[k] = item[k].decode(encoding, 'ignore')
  except:
    print traceback.format_exc(), 'with encoding:%s' % encoding
  return item


class CrawlerItem(Item):
  crawl_doc = Field()

  def to_json_str(self, include_empty=False, encodeing='utf8'):
    try:
      return json.dumps(dict(self), ensure_ascii=False).encode(encodeing)
    except:
      print 'Failed encoding json: %s, %s' % (self, traceback.format_exc())
      return None

  def get_key(self, key, type_need=str):
    try:
      if self.get(key):
        if isinstance(self[key], type_need):
          return True, self[key]
        elif type_need == str:
          return True, self[key].encode('utf-8')
        else:
          return True, type_need(self[key])
    except:
      print traceback.format_exc()
    return False, None

  def to_crawldoc(self):
    self.get('crawl_doc').crawl_time = int(time.time())
    return self['crawl_doc']


class YoutubeItem(CrawlerItem):
  channel = Field()
  category = Field()
  user = Field()
  url = Field()
  doc_id = Field()
  playlist = Field()

  def fill_item(self, response, except_key=[]):
    crawl_doc = response.meta.pop('crawl_doc')
    if not crawl_doc:
      print 'Empty crawl_doc !!!!'
      return

    self['url'] = crawl_doc.url
    self['doc_id'] = gen_docid(crawl_doc.url)
    extend_map = response.meta.get('extend_map')
    self['category'] = extend_map.get('category', None)
    self['channel'] = extend_map.get('channel', None)
    self['user'] = extend_map.get('user', None)
    self['playlist'] = extend_map.get('playlist')
    crawl_doc.id = self['doc_id']

    crawl_doc.request = Request()
    crawl_doc.request.raw_url = response.request.meta.get('Rawurl')

    crawl_doc.response = Response()
    crawl_doc.response.url = crawl_doc.url
    crawl_doc.response.body = response.body.decode(response.encoding, 'ignore').encode('utf-8')
    crawl_doc.response.header = response.headers.to_string()
    crawl_doc.response.return_code = response.status
    crawl_doc.response.redirect_info = RedirectInfo()
    crawl_doc.response.redirect_info.redirect_urls = response.meta.get('redirect_urls')
    crawl_doc.response.meta = '%s' % response.meta

    self['crawl_doc'] = encode_item(crawl_doc)

  def convert_item(self):
    data = {}
    data['doc_id'] = str(self['doc_id'])
    data['url'] = self['url']
    data['category'] = self['category']
    data['channel'] = self['channel']
    data['playlist'] = self['playlist']
    data['user'] = self['user']
    self['crawl_doc'].crawl_time = int(time.time())
    craw_doct_str = thrift_to_str(self['crawl_doc'])
    data['crawl_doc'] = base64.b64encode(craw_doct_str)
    return data


class CrawlerLoader(ItemLoader):
  default_item_class = CrawlerItem
  default_input_processor = MapCompose(lambda s: s.strip())
  default_output_processor = TakeFirst()
  description_out = Join()

