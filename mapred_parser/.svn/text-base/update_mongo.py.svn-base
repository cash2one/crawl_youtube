from pymongo import MongoClient, InsertOne, UpdateOne
from le_crawler.common.utils import str2mediavideo, get_video_attr_state
from le_crawler.proto.crawl.ttypes import CrawlStatus
from pymongo.errors import PyMongoError, OperationFailure
from pprint import pprint
import base64
import time
import commands
import time
import urllib
from python_library import utils
import logging
from logging.handlers import RotatingFileHandler
from parse_domain import parse_domain


log_name = 'update_mongo.log'
handler = RotatingFileHandler(log_name, mode='w', maxBytes=100 * 1024 * 1024, backupCount=2)
formatter = logging.Formatter('[%(levelname)s %(asctime)s %(filename)s:%(lineno)d] %(message)s')
handler.setFormatter(formatter)
handler.setLevel(logging.DEBUG)
logger = logging.getLogger(log_name)
logger.addHandler(handler)
logger.setLevel(logging.DEBUG)

out_unique_dir = 'short_video/out_unique'
out_final_dir = 'short_video/out/video/'
fs_cmd = 'hadoop fs %s'
tmp_file = 'temp_unique'
bulk_write_size = 10000


def get_files(path, last_time):
  if path == out_unique_dir:
    files = []
    job_path = ['-ls short_video/out_video_20151030_174116/parse_job/unique', '-ls short_video/out_video_20151030_174116/user_merge_job/unique']
    for cmd_str in job_path:
      cmd = fs_cmd % cmd_str
      status, result = commands.getstatusoutput(cmd)
      if status:
        logger.error('failed to run command [%s]', cmd)
        return []
      files.extend([x.split()[-1] for x in result.split('\n') if not x.startswith('Found ') and not x.endswith(' items')])
      return files

  cmd = fs_cmd % '-ls ' + path
  cmd = cmd + ' | tail -500'
  logger.debug('listing files: %s', cmd)
  status, files = commands.getstatusoutput(cmd)
  if status:
    logger.error('failed to run command [%s]', cmd)
    return []
  if not files:
    logger.info('hadoop input path is empty, [%s]', path)
    return []
  files = [x.split()[-1] for x in files.split('\n')
           if not x.startswith('Found ') and not x.endswith(' items')]
  if path == out_final_dir and last_time:
    for i in range(len(files)-1, -1, -1):
      try:
        update_time = int(files[i].split('/')[-1])
      except:
        logger.info('get file failed, filename is %(filename)s' %{'filename':files[i]})
        continue
      if update_time <= last_time:
        files = files[i+1:]
        break
  logger.info('files in [%s]:\n\t%s', path, files)
  return files


def rm_file(file_path):
  cmd = 'rm ' + file_path
  commands.getstatusoutput(cmd)


def history2dict(history):
  if not history:
    return None

  h_list = []
  if len(history.crawl_history) > 4:
    del history.crawl_history[3 : -1]
  for item in history.crawl_history:
    d = {}
    d['crawl_time'] = item.crawl_time
    d['crawl_interval'] = item.crawl_interval
    d['play_count'] = item.play_count
    h_list.append(d)
  h_dict = {}
  h_dict['crawl_history'] = h_list
  h_dict['update_time'] = history.update_time
  return h_dict 


def db_videos(file_path, collection): 
  with open(file_path, 'r') as f:
    requests = []
    req_cnt = 0
    result = None
    missing_history_cnt = 0
    missing_dict = {}
    default_status = CrawlStatus._VALUES_TO_NAMES.get(CrawlStatus.EXTRACTED)

    start_t = time.clock()
    for line in f:
      line_data = line.split('\t')
      if len(line_data) != 2:
        logger.error('schedule_info doc error: length of line incorrect.\n%s', line)
        continue
      url, base64str = line_data
      if not url:
        logger.error('schedule_info doc error: missing url.\n%s', line)
        continue
      try:
        thrift_str = base64.b64decode(base64str)
        video = str2mediavideo(thrift_str)
      except:
        logger.error('schedule_info doc error: b64decode or str2mediavideo error. %s', url)
        continue
      if not video:
        continue
      video_id = video.id
      if not video_id:
        logger.error('schedule_info doc error: missing doc_id. %s', url)
        continue
      if not video.crawl_history or not video.crawl_history.crawl_history:
        missing_history_cnt += 1
        missing_domain = parse_domain(url)
        if missing_domain not in missing_dict:
          missing_dict[missing_domain] = video
        continue
      crawl_history = history2dict(video.crawl_history)
      title = video.title
      content_timestamp = video.content_timestamp
      #update_time = time.strftime('%Y-%m-%d %H:%M:%S')
      update_time = time.time()
      attr_state = get_video_attr_state(video)
      rst_dict = {'doc_id':video_id, 'url':url, 'crawl_history':crawl_history, 'update_time':update_time,
                  'title':title, 'content_timestamp':content_timestamp, 'status': default_status, 'attr_state': attr_state}
      if req_cnt < bulk_write_size:
        requests.append(UpdateOne({'url':url}, {'$set':rst_dict}, upsert=True))
        req_cnt += 1
      else:
        try:
          result = collection.bulk_write(requests, ordered=False)
          logger.info('bulk_write result: upserted=%s matched=%s', result.upserted_count, result.matched_count)
          #print 'bulk_write result: upserted=%s matched=%s' % (result.upserted_count, result.matched_count)
        except OperationFailure, pme:
          # print pme.details
          logger.exception('update mongo bulk write error.')
        req_cnt = 0
        requests = []

    if requests:
      try:
        result = collection.bulk_write(requests, ordered=False)
        logger.info('bulk_write result: upserted=%s matched=%s', result.upserted_count, result.matched_count)
      except OperationFailure, pme:
        logger.exception('update mongo bulk write error.')
        # print pme.details
    logger.info('elapsed time: %s', time.clock() - start_t)
    logger.error('schedule_info doc error: missing crawl_history. count = %s. example below:', missing_history_cnt)
    for k, v in missing_dict.iteritems():
      logger.error('domain: %s video: %s', k, v)
    #for doc in collection.find():
    #  print doc


def process_files(collection = None):
  try:
    with open('mongo_last_update_time.txt', 'r') as f:
      line = f.read()
      if line:
        last_time = int(line)
      else:
        last_time = None
  except:
    last_time = None

  try:
    db = None
    if last_time:
      files = get_files(out_final_dir, last_time)
    else:
      files = get_files(out_unique_dir, last_time)
    if files:
      client = MongoClient('10.180.91.41:9224,10.180.91.115:9224,10.180.91.125:9224')
      client.admin.authenticate('admin', 'NjlmNTdkNGQ4OWY')
      db = client.crawl
      collection = db.schedule_info
      #print collection.count()
      #ensure_indexs(collection)
    else:
      return

    last_file = None
    for file in files:
      cmd = fs_cmd % '-text ' + file + ' > ' + tmp_file
      #print cmd
      logger.info('start processing %s ...' % file)
      status, result = commands.getstatusoutput(cmd)
      if status:
        logger.info('failed to run command [%s]\n%s', cmd, result)
        continue
      db_videos(tmp_file, collection)
      last_file = file
      if last_time and last_file:
        with open('mongo_last_update_time.txt', 'w') as f:
          last_time = last_file.split('/')[-1]
          f.write(last_time)
      logger.info('%s processed.' % file)
    rm_file(tmp_file)
    logger.info('=== job finished. ===')
  except:
    send_message()
    logger.exception('mongodb exception.')
  finally:
    if db:
      db.logout()


def send_message():
  for tel in ['13426031534', '18515029185', '15330025605']:
    api = 'http://10.182.63.85:8799/warn_messages'
    params = {}
    params['m'] = 'update_mongo exception!'
    params['p'] = tel
    params = urllib.urlencode(params)
    urllib.urlopen("%s?%s" % (api, params))


def ensure_indexs(collection):
  index_info = collection.index_information()
  #print index_info
  if not index_info or 'url_1' not in index_info or 'update_time_-1' not in index_info or 'next_schedule_time_-1' not in index_info:
    from pymongo import IndexModel, ASCENDING, DESCENDING
    logger.info('creating indexes...')
    #idx_1 = IndexModel('crawldoc_id', unique=True)
    #idx_2 = IndexModel([('update_time', DESCENDING)])
    #collection.create_indexes([idx_1, idx_2]) 
    collection.create_index('url', unique=True)
    collection.create_index([('update_time', DESCENDING)])
    collection.create_index([('next_schedule_time', DESCENDING)])
  logger.info('collection %s using index(es) with:\n%s', collection.full_name, collection.index_information())


def test():
  import pymongo
  logger.info('pymongo %s started.', pymongo.version)
  client = MongoClient('10.180.91.41:9224,10.180.91.115:9224,10.180.91.125:9224')
  client.admin.authenticate('admin', 'NjlmNTdkNGQ4OWY')
  db = client.crawl
  try:
    co = db.schedule_info
    #co.drop()
    #co.drop_indexes()
    #iensure_indexs(co)
    db_videos('unique_test', co)
    process_files()
  except Exception, e:
    print 'mongodb exception: %s' % e
  finally:
    pass
    db.logout()


if __name__ == '__main__':
  try:
    import pymongo
    logger.info('pymongo %s started.', pymongo.version)
    # utils.cycle_run(lambda: process_files(), 600)
    process_files()
  except Exception, e:
    logger.error('mongodb exception: %s', e)
    utils.send_mail('wangziqing@letv.com', 'wangziqing@letv.com', 'update mongodb failed', str(e))
