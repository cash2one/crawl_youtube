#!/usr/bin/python
# coding=utf-8

import sys
import time
import base64
# try:
#   import cPickle as pickle
# except:
#   import pickle

from le_crawler.proto.video.ttypes import OriginalUser
from le_crawler.proto.crawl.ttypes import CrawlHistory, HistoryItem
from le_crawler.common.utils import str2mediavideo, thrift2str, multi_key_fields, compress_play_trends #, int_typeids

merged_fields = set(['crawl_history', 'play_trends', 'in_links', 'user', 'page_state'])
direct_merged = set(['title', 'poster'])
need_cal_merged = set(['play_total', 'voteup_count', 'votedown_count'])
time_fields = set(['content_timestamp', 'showtime'])

class MergeItem:
  def __init__(self):
    self.reset('')


  def reset(self, url=None):
    self._all_error = True
    self._data = []
    self._url = url
    self._crawldoc_base64 = ''
    self.merged_ = False
    self.crawled_ = True
    self._user_url = None


  def get_url(self):
    return self._url


  def add_item(self, user_url, data_source, data_type, data_str, data_base64):
    self._crawldoc_base64 = data_base64
    if data_source == 'error':
      return
    if not self._url:
      sys.stderr.write('reporter:counter:reduce_error,reduce_url_empty,1\n')
      return

    if self.crawled_ and data_source != 'crawl':
      self.crawled_ = False
    if user_url != 'None':
      self._user_url = user_url
    self._all_error = False
    self._data.append(data_str)

  def _test_print(self, video):
    print '=' * 40
    print '--> Merged:', self.merged_
    for k, v in video.__dict__.iteritems():
      if v:
        print '%-20s ->' % k, v
    print '=' * 40


  def _merge_history_trends(self, videos):
    history_items = {}
    for video in videos:
      if video.crawl_history and video.crawl_history.crawl_history:
        crawl_history = video.crawl_history.crawl_history
        for item in crawl_history:
          if item.crawl_time:
            history_items[item.crawl_time] = item.play_count

    crawl_history_new = CrawlHistory()
    crawl_history_new.crawl_history = []
    # print history_items
    history_list = compress_play_trends(history_items)
    # print history_list
    history_list.reverse()
    #history_list = sorted(history_items.iteritems(), reverse=True)
    history_len = len(history_list)
    for idx, c_item in enumerate(history_list):
      item = HistoryItem()
      item.crawl_time = c_item[0]
      item.play_count = c_item[1]
      if idx + 1 < history_len:
        item.crawl_interval = c_item[0] - history_list[idx + 1][0]
      crawl_history_new.crawl_history.append(item)
      # history_list[idx] = (str(c_item[0]), str(c_item[1]))
    crawl_history_new.update_time = int(time.time())
    videos[0].crawl_history = crawl_history_new
    # videos[0].play_trends = ';'.join('|'.join(x) for x in history_list if x[1] != 'None')


  def _merge_in_links(self, videos):
    if len(videos) < 2 or not videos[0].in_links or not videos[1].in_links:
      return
    latest_anchor = videos[0].in_links[0]
    in_links = videos[1].in_links
    for idx, anchor in enumerate(in_links):
      if latest_anchor.url == anchor.url:
        in_links[idx] = latest_anchor
        break
    else:
      in_links.append(latest_anchor)
    videos[0].in_links = in_links


  def _merge_user(self, videos):
    new_user = OriginalUser()
    for k, v in new_user.__dict__.iteritems():
      for video in videos:
        if not video.user:
          continue
        old_v = getattr(video.user, k)
        if old_v:
          setattr(new_user, k, old_v)
          break
    videos[0].user = new_user


  def _merge_data(self, src, dst):
    merged = False
    for k, v in dst.__dict__.iteritems():
      src_v = getattr(src, k)
      if k in merged_fields or v == src_v:
        continue
      if not v:
        setattr(dst, k, src_v)
      elif k in time_fields:
        if src_v:  # use the old timestamp to maintain accuracy
          setattr(dst, k, src_v)
        else:
          merged = True
      elif k in direct_merged:
        merged = True
      elif k in multi_key_fields:
        v_set = set(v.split(';'))
        src_v_set = set((src_v or '').split(';'))
        if v_set - src_v_set:
          setattr(dst, k, ';'.join(list(v_set | src_v_set)))
          if k == 'category_list':
            merged = True
      elif k in need_cal_merged:
        src_v = src_v or 1
        if (v - src_v) * 1.0 / src_v > 0.1 and (v - src_v) >= 20:
          merged = True
    return merged


  def _merge_video(self, videos):
    self._merge_history_trends(videos)
    self._merge_in_links(videos)
    if self._user_url:
      self._merge_user(videos)
    # first merge crawled data
    for video in videos[1:-1]:
      self._merge_data(video, videos[0])
    # then merge the latest video and the original video
    self.merged_ = self._merge_data(videos[-1], videos[0])
    if self.merged_:
      sys.stderr.write('reporter:counter:reduce,video_updated,1\n')
      videos[0].update_time = int(time.time())


  def _print_video(self, video, data_type='video'):
    video_str = thrift2str(video)
    if not video_str:
      sys.stderr.write('reporter:counter:reduce_error,reduce_thrift2str_failed,1\n')
      return
    video_base64 = base64.b64encode(video_str)
    if not video_base64:
      sys.stderr.write('reporter:counter:reduce_error,reduce_base64encode_failed,1\n')
      return
    sys.stderr.write('reporter:counter:reduce,video_total,1\n')
    if self.crawled_:
      sys.stderr.write('reporter:counter:statistic,video_new,1\n')
    if self._user_url:
      out_type = 'video' if self.merged_ else 'unique'
      print 'user_merge' + '\t' + self._user_url + '\t' + self._url + '\t' + out_type + '\t' + video_base64
      return
    if self.merged_:
      print data_type + '\t' + self._url + '\t' + str(self._user_url) + '\t' + video_base64
    print 'unique' + '\t' + self._url + '\t' + str(self._user_url) + '\t' + video_base64


  def print_item(self):
    if not self._data:
      return
    if self._all_error:
      print 'error' + '\t' + self._url + '&error\t' + self._crawldoc_base64
      sys.stderr.write('reporter:counter:reduce,reduce_error_out,1\n')
      return
    if len(self._data) == 1:
      sys.stderr.write('reporter:counter:reduce,video_total,1\n')
      if self._user_url:
        out_type = 'video' if self.crawled_ else 'unique'
        print 'user_merge' + '\t' + self._user_url + '\t' + self._url + '\t' + out_type + '\t' + self._data[0]
        if self.crawled_:
          sys.stderr.write('reporter:counter:statistic,video_new,1\n')
        return
      print 'unique' + '\t' + self._url + '\t' + str(self._user_url) + '\t' + self._data[0]
      if self.crawled_:
        print 'video' + '\t' + self._url + '\t' + str(self._user_url) + '\t' + self._data[0]
        sys.stderr.write('reporter:counter:statistic,video_new,1\n')
      return
    for idx, data_str in enumerate(self._data):
      try:
        data = base64.b64decode(data_str)
      except:
        sys.stderr.write('reporter:counter:reduce_error,reduce_json_failed,1\n')
      data = str2mediavideo(data)
      if not data:
        sys.stderr.write('reporter:counter:reduce_error,reduce_str_to_video,1\n')
      self._data[idx] = data
    self._data = [item for item in self._data if item]
    self._data.sort(cmp=lambda x, y: (y.create_time or 0) - (x.create_time or 0))
    self._merge_video(self._data)
    self._print_video(self._data[0])


def preprocess(data_type, data, url):
  if data_type == 'debug':
    if url and data:
      print 'debug' + '\t' + url + '\t' + data
    return True
  if data_type == 'no_md5':
    if url and data:
      print 'no_md5' + '\t' + url + '\t' + data
    return True
  return False


def main():
  merge_item = MergeItem()
  url = ''
  while 1:
    line = sys.stdin.readline()
    if not line:
      break

    line_data = line.strip().split('\t', 5)
    if len(line_data) != 6:
      sys.stderr.write('reporter:counter:reduce_error,reduce_input_not_len_6,1\n')
      continue

    url, user_url, data_source, data_type, data, data_base64 = line_data
    if preprocess(data_type, data, url):
      continue
    if url == merge_item.get_url():
      merge_item.add_item(user_url, data_source, data_type, data, data_base64)
    else:
      merge_item.print_item()
      merge_item.reset(url)
      merge_item.add_item(user_url, data_source, data_type, data, data_base64)
  merge_item.print_item()


if __name__ == '__main__':
  main()

